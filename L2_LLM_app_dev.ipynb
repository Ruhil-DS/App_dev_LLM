{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55481ab3-f233-45e2-bd42-5203fb4065b1",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## outline of the module:\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8770605-d05f-4eb3-8aae-ed6df322e841",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25e4719-7c48-46d5-8ef4-7e93380552a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API Key setup\n",
    "import os\n",
    "\n",
    "api = !cat ../`ls -a ../ | grep \"gemini\"`\n",
    "\n",
    "api = api[0]\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = api\n",
    "# universal variable, used by langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe87edf4-4827-4b78-b667-18692c5396aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain\n",
    "#!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d869fe41-e2ca-4483-a03d-0d654cdc8f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatGooglePalm\n",
    "\n",
    "chat = ChatGooglePalm(temperature=0.0, model='Bison')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe3a06-fa3c-4996-b0e7-2bddbc99935b",
   "metadata": {},
   "source": [
    "# Types of Memories\n",
    "\n",
    "- Memory can be used to keep a track of the conversation\n",
    "- Let's look at various types of memories that LangChain provides us a support of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb4188b-89ac-4ddf-8042-7ba46e83df8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Previously, we used the Bison model from Google's PaLM 2 \n",
    "# We can also use gemini pro model, using this LangChain's integration package\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(temperature=0.0, model='gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467d871-d0da-47e0-a3a9-713bb06dcbfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Testing L1 knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcd5f65f-a83c-44ab-8fd1-9bd7c9f4f295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_text = \"\"\"No matter how you best understand information, our products and tools help you find answers to your health questions.\n",
    "\n",
    "YouTube’s work to bring authoritative, health-related content to more people\n",
    "At YouTube, we are working to provide people with more access to quality health information and help creators reach more audiences. Our AI-powered dubbing tool, Aloud, streamlines the video translation and dubbing process at no cost, helping bridge that gap for creators. The tool has allowed institutions, such as Mass General Brigham, to dub first-aid videos from English into Spanish, providing potentially life-saving information to more people. Now they are expanding their efforts to include videos related to chronic conditions, like COPD and cancer.\n",
    "\n",
    "We are also working to break down language barriers to bring more helpful information to health professionals. Starting today, a new animation-style course on the Stanford Medicine Continuing Medical Education YouTube channel will be available in Spanish for free. The course, which helps health professionals recognize and address implicit bias in clinical practice to better advocate for patients from underrepresented and underserved communities, was translated and dubbed using Aloud and is bridging the language gap to ultimately reach more individuals with this crucial knowledge.\n",
    "\n",
    "Deeper health insights with Fitbit\n",
    "Fitbit synthesizes your personal health and fitness data so you can track it in the Fitbit app. Our team is now building personal AI features to bring even more tailored, personalized insights that you can explore based on your unique needs and preferences.\n",
    "\n",
    "Fitbit Labs, introduced last year, will give Premium users early access to experimental AI features to test out and give feedback on. Building on Google’s AI expertise, Fitbit Labs can help you get a fuller understanding of your health data and connect the dots across that information. You will be able to ask questions in a natural way and create charts just for you to help you understand your own data better. For example, you could dig deeper into how many active zone minutes \\(AZMs\\) you get and the correlation with how restorative your sleep is.\n",
    "\n",
    "Screenshot on a phone showing example fitness data\n",
    "An example of Fitbit Labs features that will be available later this year to a limited number of Android users who are enrolled in the Fitbit Labs program in the Fitbit mobile app.\n",
    "\n",
    "Fitbit and Google Research are working together with health and wellness experts, doctors and certified coaches to create a new model for health and wellness. This new Personal Health Large Language Model will power future AI features across our portfolio.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Your job is to summarize the text delimited by the triple backticks. \\\n",
    "summarize and give the response in bullet points, if possible. \\\n",
    "\\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "lang_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_chat_input =lang_template.format_messages(text=long_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6596ea62-a0f7-498f-9c19-056cd93ca439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"**YouTube's Health-Related Content Initiatives:**\\n\\n- AI-powered dubbing tool (Aloud) streamlines video translation and dubbing, making health information accessible in multiple languages.\\n- Expansion of dubbed videos to include chronic conditions like COPD and cancer.\\n- Free Spanish-language animation course on implicit bias in clinical practice, translated and dubbed using Aloud.\\n\\n**Fitbit's Health Insights:**\\n\\n- Fitbit synthesizes personal health and fitness data for tracking in the Fitbit app.\\n- Development of personal AI features for tailored insights based on individual needs and preferences.\\n- Fitbit Labs provides early access to experimental AI features for user testing and feedback.\\n- Collaboration with health experts to create a Personal Health Large Language Model for future AI features.\" response_metadata={'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}\n"
     ]
    }
   ],
   "source": [
    "resp = chat(final_chat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6f25d27-790b-4ff3-a830-4ee30b2e5276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**YouTube's Health-Related Content Initiatives:**\n",
      "\n",
      "- AI-powered dubbing tool (Aloud) streamlines video translation and dubbing, making health information accessible in multiple languages.\n",
      "- Expansion of dubbed videos to include chronic conditions like COPD and cancer.\n",
      "- Free Spanish-language animation course on implicit bias in clinical practice, translated and dubbed using Aloud.\n",
      "\n",
      "**Fitbit's Health Insights:**\n",
      "\n",
      "- Fitbit synthesizes personal health and fitness data for tracking in the Fitbit app.\n",
      "- Development of personal AI features for tailored insights based on individual needs and preferences.\n",
      "- Fitbit Labs provides early access to experimental AI features for user testing and feedback.\n",
      "- Collaboration with health experts to create a Personal Health Large Language Model for future AI features.\n"
     ]
    }
   ],
   "source": [
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61fd974-0505-4595-b9e3-631609fec7bd",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eedec494-a01f-493a-a0f1-fe77527696be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain # To create a chain of messages (a conversation)\n",
    "from langchain.memory import ConversationBufferMemory # We'll be using the buffer memory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Dedicated LangChain package for Google Gemini\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm = chat,\n",
    "                                 memory = memory,\n",
    "                                 verbose = True\n",
    "                                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3d73d96-072e-4a2b-8805-90afaf740939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Pushpak\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Pushpak, it's nice to meet you. My name is Gemini.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Pushpak\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec0b8ed2-059a-45b4-83b9-b099d700db79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Pushpak\n",
      "AI: Hello Pushpak, it's nice to meet you. My name is Gemini.\n",
      "Human: what is LLM?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLM stands for Large Language Model. I am a type of LLM, trained by Google.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is LLM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8553ed76-0533-4a16-86d7-8cfc8af3ea24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Pushpak\n",
      "AI: Hello Pushpak, it's nice to meet you. My name is Gemini.\n",
      "Human: what is LLM?\n",
      "AI: LLM stands for Large Language Model. I am a type of LLM, trained by Google.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Pushpak.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2f0cd-4110-4710-b15e-a18916da9be1",
   "metadata": {},
   "source": [
    "---\n",
    "- It can be seen that the entire conversation is being stored in a memory\n",
    "- This way, the conversation messages shared can be retrieved as a variable\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6be2eb6-f4c8-4de7-bebc-49b9d04e5238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6454689-b28d-404d-a4b6-fb66b30eedd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Pushpak.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6745f-92f2-4d1b-8d96-46beb55c3dfa",
   "metadata": {},
   "source": [
    "---\n",
    "**We can also Display the memory buffer as it is**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0132ee22-5a5a-480a-9627-ffb001c96e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Pushpak\n",
      "AI: Hello Pushpak, it's nice to meet you. My name is Gemini.\n",
      "Human: what is LLM?\n",
      "AI: LLM stands for Large Language Model. I am a type of LLM, trained by Google.\n",
      "Human: What is my name?\n",
      "AI: Your name is Pushpak.\n",
      "Human: What is my name?\n",
      "AI: Your name is Pushpak.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88754e81-d188-4055-8cc0-5dd387097658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: Hi, my name is Pushpak\\nAI: Hello Pushpak, it's nice to meet you. My name is Gemini.\\nHuman: what is LLM?\\nAI: LLM stands for Large Language Model. I am a type of LLM, trained by Google.\\nHuman: What is my name?\\nAI: Your name is Pushpak.\\nHuman: What is my name?\\nAI: Your name is Pushpak.\"}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19440b64-1154-4a6b-9878-ed94b14a6fac",
   "metadata": {},
   "source": [
    "---\n",
    "**We can also save some prior context to the memory**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d5dbb2a-e085-448f-85ab-6282228bcfed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"what's up, I am Pushpak's personal bot and my name is RuhilBot. How may I assist you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd299a44-5ebc-4cc9-90bb-baed9838beb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: what's up, I am Pushpak's personal bot and my name is RuhilBot. How may I assist you?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a251bcfe-d83f-4063-bf31-a49ce348d3f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: what's up, I am Pushpak's personal bot and my name is RuhilBot. How may I assist you?\"}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f9c5dc1-1071-4d87-86b0-20537acd3379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0cff2eb-f013-4ffb-a283-dcfbc4a3de01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: what's up, I am Pushpak's personal bot and my name is RuhilBot. How may I assist you?\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c592f9b-16fc-4a04-af8a-1288f0f05e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm = chat,\n",
    "                                 memory = memory,\n",
    "                                 verbose = False\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64061e02-0561-4892-aa9b-4187a9dcda26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi\\nAI: what's up, I am Pushpak's personal bot and my name is RuhilBot. How may I assist you?\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: Not much, just hanging\\nAI: Cool\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "afd483a5-0722-40f7-a0fd-b87208a1669e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RuhilBot'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What's your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afd2de-b325-4d23-96cd-5edf3cdb76bb",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory\n",
    "\n",
    "- As the name suggests, it's similar to buffer memory but keeps a window is some \"k\" context messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f40ae201-d45c-44bf-ba3f-f154b29e35dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf702c8a-d465-4d31-9ef5-3fe91544c4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "memory.save_context({\"input\": \"My name is Pushpak\"}, {\"output\": \"Hi Pushpak, what's up?\"})\n",
    "\n",
    "conversation = ConversationChain(llm = chat,\n",
    "                                 memory = memory,\n",
    "                                 verbose = True\n",
    "                                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a3bb04ba-296e-4a74-a626-52bd7a1f0d96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: My name is Pushpak\n",
      "AI: Hi Pushpak, what's up?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "63939eb0-a811-455a-8a43-36aae73fc7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: My name is Pushpak\n",
      "AI: Hi Pushpak, what's up?\n",
      "Human: What's my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pushpak'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7dc0585f-9337-4e63-8232-3e8ec86d2e52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What's my name?\n",
      "AI: Pushpak\n",
      "Human: What is the full form of GPT, BERT, LLM?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'* GPT stands for Generative Pre-trained Transformer.\\n* BERT stands for Bidirectional Encoder Representations from Transformers.\\n* LLM stands for Large Language Model.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the full form of GPT, BERT, LLM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1eb0a738-2477-4744-878a-2cb0e850b54d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is the full form of GPT, BERT, LLM?\n",
      "AI: * GPT stands for Generative Pre-trained Transformer.\n",
      "* BERT stands for Bidirectional Encoder Representations from Transformers.\n",
      "* LLM stands for Large Language Model.\n",
      "Human: What's my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I do not have access to that information.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What's my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d48259-b1b9-4181-8846-aef5b89c898c",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n",
    "\n",
    "- This, again, keeps a buffer but at the same time, the \"window\" is decided by the token limit\n",
    "- uses token length rather than number of interactions to determine when to flush interactions.\n",
    "- Token size varies from LLM to LLM\n",
    "- So, we specify the LLM in the memory itself\n",
    "- For example, google's gemini has 4 characters/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9febf902-b08a-458b-9094-203e7144a139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "485e4e58-5bd3-4fab-ac5d-5f47e0431de2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'AI: Charming!'}\n",
      "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}\n"
     ]
    }
   ],
   "source": [
    "# Notice how the llm is passed in the memory here\n",
    "memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=10)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "#---------\n",
    "memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=50)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa9741-3843-40b1-b9bf-21733cb459ca",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "\n",
    "- Here, we store the summary of the conversation\n",
    "- Note that each message will have an additional API call to summarize the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c597f335-48b6-442b-ac6e-ed36244fa2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d139fb51-6b48-4049-b77a-9b635fce91bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "# Again, we provide the LLM in the memory variable\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=100)\n",
    "\n",
    "# Saving some context\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34930d48-d12f-4eb8-b87b-e7712df74879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human says hello to the AI. The AI asks what's up. The human says not much, just hanging. The AI says cool. The human asks what is on the schedule today.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35bdc90f-a7f1-4431-ad70-34dffd4a67a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=chat, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c993e963-2ee0-4dd0-87cd-2fa0ee22258e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human says hello to the AI. The AI asks what's up. The human says not much, just hanging. The AI says cool. The human asks what is on the schedule today.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I would recommend showing the LLM's ability to generate creative content, such as a poem or a story. You could also show its ability to answer complex questions or translate languages. Additionally, you could demonstrate its ability to generate code or write different kinds of text, such as marketing copy or legal documents.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0c4f4ea3-80b3-4d30-8d57-7291d666252b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human says hello to the AI. The AI asks what's up. The human says not much, just hanging. The AI says cool. The human asks what is on the schedule today. The AI says there is a meeting at 8am with the product team, time to work on the LangChain project from 9am-12pm, and lunch at an Italian restaurant with a customer at Noon.\\nHuman: What would be a good demo to show?\\nAI: I would recommend showing the LLM's ability to generate creative content, such as a poem or a story. You could also show its ability to answer complex questions or translate languages. Additionally, you could demonstrate its ability to generate code or write different kinds of text, such as marketing copy or legal documents.\"}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1cd76-3c0c-4321-b14e-a4fa3c6d9481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
